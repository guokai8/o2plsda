% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sparse_o2pls.R
\name{sparse_o2pls}
\alias{sparse_o2pls}
\title{Sparse Two-way Orthogonal Partial Least Squares}
\usage{
sparse_o2pls(
  X,
  Y,
  nc,
  nx = 0,
  ny = 0,
  keepX = NULL,
  keepY = NULL,
  lambda_x = 0.1,
  lambda_y = 0.1,
  penalty = "lasso",
  scale = TRUE,
  center = TRUE,
  max_iter = 100,
  tol = 1e-06
)
}
\arguments{
\item{X}{Numeric matrix (samples x variables). The predictor data matrix.}

\item{Y}{Numeric matrix (samples x variables). The response data matrix.}

\item{nc}{Integer. Number of joint components to extract.}

\item{nx}{Integer. Number of X-specific orthogonal components (default: 0).}

\item{ny}{Integer. Number of Y-specific orthogonal components (default: 0).}

\item{keepX}{Vector of integers. Number of X variables to keep per component.
If NULL, automatically determined based on data dimensions.}

\item{keepY}{Vector of integers. Number of Y variables to keep per component.
If NULL, automatically determined based on data dimensions.}

\item{lambda_x}{Numeric. L1 penalty parameter for X variables (default: 0.1).}

\item{lambda_y}{Numeric. L1 penalty parameter for Y variables (default: 0.1).}

\item{penalty}{Character. Penalty type: "lasso" (default) or "elastic".}

\item{scale}{Logical. Scale variables to unit variance (default: TRUE).}

\item{center}{Logical. Center variables to zero mean (default: TRUE).}

\item{max_iter}{Integer. Maximum iterations for convergence (default: 100).}

\item{tol}{Numeric. Convergence tolerance (default: 1e-6).}
}
\value{
sparse_o2pls object containing sparse loadings, scores, and sparsity information.
}
\description{
Performs sparse O2PLS analysis with automatic variable selection using
L1 regularization (Lasso) or other penalty methods.
}
\details{
Sparse O2PLS extends traditional O2PLS by incorporating variable selection
through L1 regularization. This is particularly useful for high-dimensional
data where only a subset of variables are relevant for the relationship
between X and Y datasets.

The keepX and keepY parameters control the level of sparsity. Smaller values
lead to sparser models with fewer selected variables. If not specified,
reasonable defaults are chosen based on data dimensions.
}
\examples{
# Example 1: Basic sparse O2PLS with automatic parameter selection
set.seed(42)
n <- 80
p_X <- 100  # High-dimensional X
p_Y <- 60   # High-dimensional Y

# Generate sparse data with only some variables being relevant
X <- matrix(rnorm(n * p_X), n, p_X)
Y <- matrix(rnorm(n * p_Y), n, p_Y)

# Create relationships between first 20 variables
true_signal <- matrix(rnorm(n * 3), n, 3)  # 3 latent factors
X[, 1:20] <- true_signal \%*\% matrix(rnorm(3 * 20), 3, 20) + 
             matrix(rnorm(n * 20, sd = 0.5), n, 20)
Y[, 1:15] <- true_signal \%*\% matrix(rnorm(3 * 15), 3, 15) + 
             matrix(rnorm(n * 15, sd = 0.5), n, 15)

# Add variable names
colnames(X) <- paste0("X_", 1:p_X)
colnames(Y) <- paste0("Y_", 1:p_Y)
rownames(X) <- rownames(Y) <- paste0("Sample_", 1:n)

# Fit sparse O2PLS with automatic keepX/keepY selection
sparse_fit1 <- sparse_o2pls(X, Y, nc = 2)

# View results
print(sparse_fit1)
summary(sparse_fit1)

# Extract selected variables
selected_X <- selected_vars(sparse_fit1, type = "X")
selected_Y <- selected_vars(sparse_fit1, type = "Y")
selected_X_names <- selected_var_names(sparse_fit1, type = "X")
selected_Y_names <- selected_var_names(sparse_fit1, type = "Y")

cat("Selected X variables:", length(selected_X), "out of", p_X, "\n")
cat("Selected Y variables:", length(selected_Y), "out of", p_Y, "\n")
cat("X variables:", paste(head(selected_X_names, 10), collapse = ", "), "\n")

# Example 2: Controlled sparsity levels
# Specify exact number of variables to keep
sparse_fit2 <- sparse_o2pls(
    X = X, 
    Y = Y,
    nc = 2,
    nx = 1,
    ny = 1, 
    keepX = c(25, 20),     # Keep 25 variables in comp 1, 20 in comp 2
    keepY = c(20, 15),     # Keep 20 variables in comp 1, 15 in comp 2
    lambda_x = 0.05,
    lambda_y = 0.05,
    penalty = "lasso"
)

# Compare sparsity levels
sparsity_info1 <- sparsity_info(sparse_fit1)
sparsity_info2 <- sparsity_info(sparse_fit2)

print("Automatic sparsity:")
print(sparsity_info1)
print("Controlled sparsity:")
print(sparsity_info2)

# Example 3: Prediction with sparse model
# Split data for validation
train_idx <- 1:60
test_idx <- 61:80

X_train <- X[train_idx, ]
Y_train <- Y[train_idx, ]
X_test <- X[test_idx, ]
Y_test <- Y[test_idx, ]

# Fit on training data
sparse_train <- sparse_o2pls(X_train, Y_train, nc = 2, keepX = c(30, 25))

# Predict on test data
Y_pred <- predict(sparse_train, X_test)

# Calculate prediction error
pred_error <- sqrt(mean((Y_test - Y_pred)^2))
cat("Prediction RMSE:", round(pred_error, 4), "\n")

# Example 4: Different penalty types
# Compare Lasso vs Elastic Net
sparse_lasso <- sparse_o2pls(X, Y, nc = 2, penalty = "lasso", lambda_x = 0.1)
sparse_elastic <- sparse_o2pls(X, Y, nc = 2, penalty = "elastic", lambda_x = 0.1)

# Compare number of selected variables
cat("Lasso selected:", length(selected_vars(sparse_lasso, "X")), "X variables\n")
cat("Elastic Net selected:", length(selected_vars(sparse_elastic, "X")), "X variables\n")

}
\seealso{
\code{\link{o2pls}}, \code{\link{selected_vars}}, \code{\link{stability_selection}}
}
\author{
Kai Guo
}
