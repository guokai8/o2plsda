% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sparse_plsda.R
\name{sparse_plsda}
\alias{sparse_plsda}
\title{Sparse Partial Least Squares Discriminant Analysis}
\usage{
sparse_plsda(
  X,
  Y,
  nc,
  keepX = NULL,
  validation = "Mfold",
  folds = 5,
  test.keepX = seq(5, 50, 5),
  scale = TRUE,
  center = TRUE,
  dist = "max.dist",
  tune.keepX = is.null(keepX)
)
}
\arguments{
\item{X}{Numeric matrix (samples x variables). The predictor data matrix.}

\item{Y}{Factor or character vector. Class labels for each sample.}

\item{nc}{Integer. Number of components to extract.}

\item{keepX}{Vector of integers. Number of variables to keep per component.
If NULL, automatically determined.}

\item{validation}{Character. Validation method for parameter tuning: "Mfold" or "loo".}

\item{folds}{Integer. Number of cross-validation folds (default: 5).}

\item{test.keepX}{Vector of keepX values to test during tuning.}

\item{scale}{Logical. Scale variables (default: TRUE).}

\item{center}{Logical. Center variables (default: TRUE).}

\item{dist}{Character. Distance metric for classification: "max.dist", 
"centroids.dist", or "mahalanobis.dist".}

\item{tune.keepX}{Logical. Automatically tune keepX parameters (default: TRUE if keepX is NULL).}
}
\value{
sparse_plsda object containing sparse loadings, scores, VIP scores, and classification results.
}
\description{
Performs sparse PLS-DA for classification with automatic variable selection.
}
\examples{
# Example 1: Basic sparse PLS-DA
set.seed(123)
n <- 120
p <- 200  # High-dimensional feature space

# Generate classification data
X <- matrix(rnorm(n * p), n, p)

# Create three classes with different patterns
n_per_class <- n / 3
classes <- rep(c("Class_A", "Class_B", "Class_C"), each = n_per_class)

# Add class-specific signals to subsets of variables
X[classes == "Class_A", 1:20] <- X[classes == "Class_A", 1:20] + 2
X[classes == "Class_B", 21:40] <- X[classes == "Class_B", 21:40] + 2
X[classes == "Class_C", 41:60] <- X[classes == "Class_C", 41:60] + 2

# Add some noise correlation
X[, 61:80] <- X[, 1:20] + matrix(rnorm(n * 20, sd = 0.5), n, 20)

colnames(X) <- paste0("Feature_", 1:p)
Y <- factor(classes)

# Fit sparse PLS-DA with automatic tuning
sparse_plsda_fit <- sparse_plsda(
    X = X,
    Y = Y,
    nc = 2,
    validation = "Mfold",
    folds = 5,
    test.keepX = seq(10, 50, 10),  # Test different sparsity levels
    tune.keepX = TRUE
)

# View results
print(sparse_plsda_fit)
summary(sparse_plsda_fit)

# Extract selected features
selected_features <- selected_vars(sparse_plsda_fit)
selected_names <- selected_var_names(sparse_plsda_fit)

cat("Selected", length(selected_features), "out of", p, "features\n")
cat("Selected features:", paste(head(selected_names, 15), collapse = ", "), "\n")

# Plot results
plot(sparse_plsda_fit, type = "score", group = Y)
plot(sparse_plsda_fit, type = "loading", top = 20)

# Example 2: Manual keepX specification
sparse_plsda_manual <- sparse_plsda(
    X = X,
    Y = Y,
    nc = 3,
    keepX = c(30, 25, 20),  # Decreasing sparsity per component
    tune.keepX = FALSE,
    dist = "centroids.dist"
)

# Compare classification performance
cat("Auto-tuned error rate:", sparse_plsda_fit$classification$overall_error, "\n")
cat("Manual keepX error rate:", sparse_plsda_manual$classification$overall_error, "\n")

# Example 3: Cross-validation and prediction
# Split data
train_idx <- sample(1:n, 0.7 * n)
test_idx <- setdiff(1:n, train_idx)

X_train <- X[train_idx, ]
Y_train <- Y[train_idx]
X_test <- X[test_idx, ]
Y_test <- Y[test_idx]

# Train model
model_train <- sparse_plsda(X_train, Y_train, nc = 2, keepX = c(25, 20))

# Predict test set
predictions <- predict(model_train, X_test, dist = "max.dist")

# Evaluate predictions
confusion_matrix <- table(Predicted = predictions$class, Actual = Y_test)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

print(confusion_matrix)
cat("Test set accuracy:", round(accuracy, 3), "\n")

}
\seealso{
\code{\link{sparse_o2pls}}, \code{\link{stability_selection}}, \code{\link{tune_sparse_keepX}}
}
\author{
Kai Guo
}
